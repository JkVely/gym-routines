{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM\n",
    "vamos a crear un llm usando python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "instalamos los siguientes requerimientos: (!pip para que lo instale en este cuaderno):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch] datasets fastapi faiss-cpu uvicorn\n",
    "%pip install pymupdf\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install numpy\n",
    "%pip install ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer la informacion del pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Abre el archivo PDF\n",
    "    document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Extrae texto de cada página\n",
    "    text = \"\"\n",
    "    for page_num in range(document.page_count):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text(\"text\")\n",
    "    \n",
    "    # Cierra el documento\n",
    "    document.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generamos un modelo fundacional usando finetunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 15.12 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=2.1572252909342446, metrics={'train_runtime': 16.0266, 'train_samples_per_second': 0.187, 'train_steps_per_second': 0.187, 'total_flos': 789354427392.0, 'train_loss': 2.1572252909342446, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load foundational model\n",
    "model_name = \"google-bert/bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# prepare specific data\n",
    "text_concepts = extract_text_from_pdf(\"docs/concepts.pdf\")\n",
    "dataset = Dataset.from_dict({\"text\": [text_concepts], \"labels\": [0]})  # Add a dummy label\n",
    "\n",
    "# tokenize the data\n",
    "def tokenize_function(concepts):\n",
    "    return tokenizer(concepts[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])  # Set format for PyTorch\n",
    "\n",
    "# train model with fine-tuning\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "fine-tunning le da un conocimiento base que no se modifica. RAG es darle los conceptos modernos o actualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embed() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;241m768\u001b[39m)\n\u001b[0;32m      8\u001b[0m documents \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs/upgrades.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents])\n\u001b[0;32m     10\u001b[0m index\u001b[38;5;241m.\u001b[39madd(embeddings)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# load generator\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: embed() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "from IPython import embed\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# load retriever\n",
    "index = faiss.IndexFlatIP(768)\n",
    "documents = [\"docs/upgrades.pdf\"]\n",
    "embeddings = np.array([embed(doc) for doc in documents])\n",
    "index.add(embeddings)\n",
    "\n",
    "# load generator\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# funcion para obtener info\n",
    "def retrieve(query: str) -> str:\n",
    "    # embed query\n",
    "    query_embedding = embed(query)\n",
    "    \n",
    "    # retrieve\n",
    "    D, I = index.search(np.array([query_embedding]), 1)\n",
    "    \n",
    "    return [documents[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(promt: str) -> str:\n",
    "    retrieved_document = retrieve(promt)\n",
    "    context = \" \".join(retrieved_document)\n",
    "    input_text = f\"{context} {promt}\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    output = model.generate(inputs, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# example\n",
    "promt = \"What is the difference between a concept and a feature?\"\n",
    "response = generate_response(promt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Query, app\n",
    "\n",
    "\n",
    "@app.post(\"/chatbot\")\n",
    "def chatbot(query: Query):\n",
    "    try:\n",
    "        response = generate_response(query.promt)\n",
    "        return {\"response\": response}\n",
    "    except Exception as e:\n",
    "        return {\"response\": str(e)}\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
